{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "FOR PAGE 1"
      ],
      "metadata": {
        "id": "EEjRzqEXYvXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# CONFIGURATION\n",
        "# ---------------------------------------------------------\n",
        "INPUT_PATH = \"olim_2015-2024_preprocessed.csv\" # Your raw 'single sightings' file\n",
        "OUTPUT_PATH = \"olim_aggregated.csv\"             # The new optimized file\n",
        "\n",
        "def create_aggregated_dataset():\n",
        "    print(f\"Loading raw data from {INPUT_PATH}...\")\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(INPUT_PATH)\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: Input file not found. Please check the path.\")\n",
        "        return\n",
        "\n",
        "    original_rows = len(df)\n",
        "    print(f\"Original row count: {original_rows:,}\")\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 1. CLEANING (Logic copied from your Streamlit app)\n",
        "    # ---------------------------------------------------------\n",
        "    print(\"Cleaning data...\")\n",
        "\n",
        "    # Standardize country names\n",
        "    if \"erez_moza\" in df.columns:\n",
        "        df[\"erez_moza\"] = df[\"erez_moza\"].astype(str).str.strip()\n",
        "\n",
        "    # Ensure numeric years/months\n",
        "    df[\"year_aliya\"] = pd.to_numeric(df[\"year_aliya\"], errors='coerce')\n",
        "    df[\"month_aliya\"] = pd.to_numeric(df[\"month_aliya\"], errors='coerce')\n",
        "    df = df.dropna(subset=[\"year_aliya\", \"month_aliya\"])\n",
        "\n",
        "    # Create Date column\n",
        "    df[\"date\"] = pd.to_datetime(\n",
        "        df[\"year_aliya\"].astype(int).astype(str) + \"-\" +\n",
        "        df[\"month_aliya\"].astype(int).astype(str) + \"-01\",\n",
        "        errors=\"coerce\"\n",
        "    )\n",
        "    df = df.dropna(subset=[\"date\"])\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 2. AGGREGATION\n",
        "    # ---------------------------------------------------------\n",
        "    print(\"Aggregating data...\")\n",
        "\n",
        "    # Group by Date, Country, and Continent to get counts\n",
        "    # We maintain 'continent' here so we don't lose it in the app\n",
        "    aggregated_df = (\n",
        "        df.groupby([\"date\", \"erez_moza\", \"continent\"], as_index=False)\n",
        "        .size()\n",
        "        .rename(columns={\"size\": \"monthly_count\"})\n",
        "    )\n",
        "\n",
        "    # Sort for cleaner usage later\n",
        "    aggregated_df = aggregated_df.sort_values(by=[\"date\", \"erez_moza\"])\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 3. SAVING\n",
        "    # ---------------------------------------------------------\n",
        "    new_rows = len(aggregated_df)\n",
        "    reduction = (1 - (new_rows / original_rows)) * 100\n",
        "\n",
        "    print(f\"Aggregated row count: {new_rows:,}\")\n",
        "    print(f\"Data reduction: {reduction:.2f}%\")\n",
        "\n",
        "    aggregated_df.to_csv(OUTPUT_PATH, index=False)\n",
        "    print(f\"File saved successfully to: {OUTPUT_PATH}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    create_aggregated_dataset()"
      ],
      "metadata": {
        "id": "uB7uGGesmbpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JVAUBmNXH4I"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- CONFIGURATION: UPDATE THESE PATHS ---\n",
        "PATH_OLIM = \"/content/olim_aggregated.csv\"\n",
        "PATH_GDP = \"/content/gdp.csv\"\n",
        "OUTPUT_FILENAME = \"page1_final.csv\"\n",
        "\n",
        "def create_aggregated_file():\n",
        "    print(\"Loading data...\")\n",
        "\n",
        "    # 1. Load and Clean Olim Data\n",
        "    try:\n",
        "        # Assuming olim file has columns: date, erez_moza, monthly_count, continent\n",
        "        df_olim = pd.read_csv(PATH_OLIM, parse_dates=[\"date\"])\n",
        "        # Ensure date is normalized to start of month just in case\n",
        "        df_olim[\"date\"] = df_olim[\"date\"] + pd.offsets.MonthBegin(0)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading Olim data: {e}\")\n",
        "        return\n",
        "\n",
        "    # 2. Load and Process GDP Data\n",
        "    try:\n",
        "        df_gdp_raw = pd.read_csv(PATH_GDP)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading GDP data: {e}\")\n",
        "        return\n",
        "\n",
        "    # Clean GDP\n",
        "    if \"erez_moza\" in df_gdp_raw.columns:\n",
        "        df_gdp_raw = df_gdp_raw.dropna(subset=[\"erez_moza\"])\n",
        "        df_gdp_raw[\"erez_moza\"] = df_gdp_raw[\"erez_moza\"].astype(str).str.strip()\n",
        "\n",
        "    # Extract Country Name Mapping (Hebrew -> English)\n",
        "    # We will merge this back later to ensure every row has the English name\n",
        "    hebrew_to_english_df = df_gdp_raw[[\"erez_moza\", \"Country\"]].dropna().drop_duplicates(subset=[\"erez_moza\"])\n",
        "\n",
        "    # Melt GDP (Years to Rows)\n",
        "    year_cols = [c for c in df_gdp_raw.columns if c.isdigit()]\n",
        "    gdp_melt = df_gdp_raw.melt(\n",
        "        id_vars=[\"erez_moza\"],\n",
        "        value_vars=year_cols,\n",
        "        var_name=\"year\",\n",
        "        value_name=\"gdp\"\n",
        "    )\n",
        "    gdp_melt[\"year\"] = pd.to_numeric(gdp_melt[\"year\"])\n",
        "    gdp_melt[\"gdp\"] = pd.to_numeric(gdp_melt[\"gdp\"], errors='coerce')\n",
        "\n",
        "    # Create Date column (Jan 1st of every year)\n",
        "    gdp_melt[\"date\"] = pd.to_datetime(gdp_melt[\"year\"].astype(str) + \"-01-01\")\n",
        "    gdp_melt = gdp_melt.dropna(subset=[\"gdp\", \"date\"])\n",
        "    gdp_melt = gdp_melt.sort_values([\"erez_moza\", \"date\"])\n",
        "\n",
        "    # Interpolate GDP to Monthly level\n",
        "    print(\"Interpolating GDP data (this might take a moment)...\")\n",
        "    gdp_interp = (\n",
        "        gdp_melt.set_index(\"date\")\n",
        "        .groupby(\"erez_moza\")[\"gdp\"]\n",
        "        .apply(lambda x: x.resample(\"MS\").interpolate(method=\"linear\"))\n",
        "        .reset_index()\n",
        "    )\n",
        "\n",
        "    # 3. Merge Datasets\n",
        "    # We use an outer join to keep GDP data even if no immigration happened that month, and vice versa\n",
        "    print(\"Merging datasets...\")\n",
        "    df_merged = pd.merge(\n",
        "        df_olim,\n",
        "        gdp_interp,\n",
        "        on=[\"date\", \"erez_moza\"],\n",
        "        how=\"outer\"\n",
        "    )\n",
        "\n",
        "    # 4. Cleanup Merged Data\n",
        "    # Fill NaNs for monthly_count with 0 (if a month exists in GDP but not in Olim, count is 0)\n",
        "    df_merged[\"monthly_count\"] = df_merged[\"monthly_count\"].fillna(0)\n",
        "\n",
        "    # Forward fill Continents and English Names per country\n",
        "    # (Because rows coming purely from GDP might miss the continent info from the Olim file)\n",
        "\n",
        "    # First, merge the English names we extracted earlier\n",
        "    df_merged = pd.merge(df_merged, hebrew_to_english_df, on=\"erez_moza\", how=\"left\")\n",
        "\n",
        "    # Now fix missing continents. We group by country and fill forward/backward\n",
        "    if \"continent\" in df_merged.columns:\n",
        "        df_merged[\"continent\"] = df_merged.groupby(\"erez_moza\")[\"continent\"].transform(lambda x: x.ffill().bfill())\n",
        "\n",
        "    # Drop rows where we still don't have a continent (usually dirty data) or Date\n",
        "    df_merged = df_merged.dropna(subset=[\"date\", \"continent\"])\n",
        "\n",
        "    # --- NEW: HARD CUTOFF FOR DATE ---\n",
        "    # This ensures we don't get January 2025 just because of GDP interpolation\n",
        "    df_merged = df_merged[df_merged[\"date\"] <= \"2024-12-01\"]\n",
        "    # ---------------------------------\n",
        "\n",
        "    # Sort\n",
        "    df_merged = df_merged.sort_values(by=[\"erez_moza\", \"date\"])\n",
        "\n",
        "    # 5. Save\n",
        "    print(f\"Saving to {OUTPUT_FILENAME}...\")\n",
        "    df_merged.to_csv(OUTPUT_FILENAME, index=False)\n",
        "    print(\"Done! You can now upload this file.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    create_aggregated_file()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FOR PAGE 2"
      ],
      "metadata": {
        "id": "AG4rC6_AmE25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def create_aggregated_dataset():\n",
        "    print(\"--- 1. Loading Data ---\")\n",
        "    try:\n",
        "        # Load the Raw Olim Data\n",
        "        df_raw = pd.read_csv(\"olim_2015-2024_preprocessed.csv\")\n",
        "\n",
        "        # Load the Mapping File\n",
        "        df_map = pd.read_csv(\"mapped_yeshuvim.csv\")\n",
        "        df_map['mapped_district'] = df_map['mapped_district'].astype(str).str.strip()\n",
        "\n",
        "        # Load the Scores File\n",
        "        df_scores = pd.read_csv(\"isr_data.csv\")\n",
        "        df_scores['english'] = df_scores['english'].astype(str).str.strip()\n",
        "        # Convert scores to numeric, forcing errors to NaN then filling with 0\n",
        "        df_scores['score'] = pd.to_numeric(df_scores['score'], errors='coerce').fillna(0)\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error: Could not find file {e.filename}\")\n",
        "        return\n",
        "\n",
        "    print(\"--- 2. Preprocessing Raw Data ---\")\n",
        "\n",
        "    # A. Employment Status Logic\n",
        "    # subject: 'לא עבד' or 'לא צויין' -> Not Employed (0). Others -> Employed (1)\n",
        "    if 'subject' in df_raw.columns:\n",
        "        df_raw['is_employed'] = (~df_raw['subject'].isin(['לא עבד', 'לא צויין'])).astype(int)\n",
        "    else:\n",
        "        print(\"Warning: 'subject' column missing. simulating employment data.\")\n",
        "        df_raw['is_employed'] = np.random.randint(0, 2, size=len(df_raw))\n",
        "\n",
        "    # B. Age Logic\n",
        "    # Calculate Age from Birth Year (assuming current year 2025)\n",
        "    if 'age' not in df_raw.columns:\n",
        "        if 'birth_year' in df_raw.columns:\n",
        "            df_raw['age'] = 2025 - df_raw['birth_year']\n",
        "        else:\n",
        "            print(\"Warning: 'age' and 'birth_year' missing. Simulating ages.\")\n",
        "            df_raw['age'] = np.random.randint(18, 90, size=len(df_raw))\n",
        "\n",
        "    # C. Gender Logic (Convert to numeric for averaging)\n",
        "    # 1 = Female, 0 = Male\n",
        "    if 'gender' in df_raw.columns:\n",
        "        df_raw['is_female'] = (df_raw['gender'] == 'נקבה').astype(int)\n",
        "    else:\n",
        "        df_raw['is_female'] = 0.5 # Fallback\n",
        "\n",
        "    print(\"--- 3. Aggregating Data ---\")\n",
        "\n",
        "    # Group by Hebrew Name ('yeshuv_klita')\n",
        "    # We calculate: Count (Total), Mean Age, Mean Employment, Mean Female %\n",
        "    df_agg = df_raw.groupby('yeshuv_klita').agg(\n",
        "        total_olim=('age', 'count'),\n",
        "        avg_age=('age', 'mean'),\n",
        "        pct_employed=('is_employed', 'mean'),\n",
        "        pct_female=('is_female', 'mean')\n",
        "    ).reset_index()\n",
        "\n",
        "    # Convert fractions (0.5) to Percentages (50.0)\n",
        "    df_agg['pct_employed'] = (df_agg['pct_employed'] * 100).round(1)\n",
        "    df_agg['pct_female'] = (df_agg['pct_female'] * 100).round(1)\n",
        "    df_agg['pct_male'] = (100 - df_agg['pct_female']).round(1)\n",
        "    df_agg['avg_age'] = df_agg['avg_age'].round(1)\n",
        "\n",
        "    print(\"--- 4. Merging with Metadata (IDs and Scores) ---\")\n",
        "\n",
        "    # Merge 1: Add English IDs from mapping file\n",
        "    # We join on the Hebrew name\n",
        "    df_merged = df_agg.merge(\n",
        "        df_map[['hebrew_name', 'mapped_district']],\n",
        "        left_on='yeshuv_klita',\n",
        "        right_on='hebrew_name',\n",
        "        how='inner' # Keep only cities we have a map for\n",
        "    )\n",
        "\n",
        "    # Merge 2: Add Scores from isr_data\n",
        "    # We join on the English ID ('mapped_district' vs 'english')\n",
        "    df_final = df_merged.merge(\n",
        "        df_scores[['english', 'score']],\n",
        "        left_on='mapped_district',\n",
        "        right_on='english',\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "    # Rename columns for clarity in the app\n",
        "    df_final = df_final.rename(columns={\n",
        "        'mapped_district': 'english_id',\n",
        "        'hebrew_name': 'hebrew_name'\n",
        "    })\n",
        "\n",
        "    # Fill missing scores with 0\n",
        "    df_final['score'] = df_final['score'].fillna(0)\n",
        "\n",
        "    # Clean up columns (drop duplicates/helper cols)\n",
        "    cols_to_keep = ['english_id', 'hebrew_name', 'total_olim', 'avg_age', 'pct_employed', 'pct_female', 'pct_male', 'score']\n",
        "    df_final = df_final[cols_to_keep]\n",
        "\n",
        "    print(f\"--- 5. Saving to CSV ({len(df_final)} cities) ---\")\n",
        "    df_final.to_csv(\"olim_aggregated_ready.csv\", index=False)\n",
        "    print(\"Done! File 'olim_aggregated_ready.csv' created successfully.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    create_aggregated_dataset()"
      ],
      "metadata": {
        "id": "g2Ix1Dd_mGBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. Load the datasets\n",
        "olim_df = pd.read_csv('olim_aggregated_ready.csv')\n",
        "isr_df = pd.read_csv('isr_data.csv')\n",
        "\n",
        "# 2. Merge the dataframes\n",
        "# We map 'english_id' from the olim dataset to the 'english' column in the israel data\n",
        "merged_df = olim_df.merge(\n",
        "    isr_df[['english', 'madad']],\n",
        "    left_on='english_id',\n",
        "    right_on='english',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# 3. Clean up (remove the duplicate 'english' column)\n",
        "merged_df = merged_df.drop(columns=['english'])\n",
        "\n",
        "# 4. Save the result\n",
        "merged_df.to_csv('page2_final.csv', index=False)"
      ],
      "metadata": {
        "id": "53GekSTAmJku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FOR PAGE 3"
      ],
      "metadata": {
        "id": "sRgAYtDemLgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. Setup Paths\n",
        "PATH_RAW_DATA = '/content/olim_2015-2024_preprocessed.csv'\n",
        "OUTPUT_PATH = 'page3_final.csv'\n",
        "\n",
        "def process_and_save_data():\n",
        "    print(\"Loading raw data...\")\n",
        "    df = pd.read_csv(PATH_RAW_DATA)\n",
        "\n",
        "    # 2. Aggregation / Mapping\n",
        "    # (Applying this first, exactly as in your original script)\n",
        "    mapping = {\n",
        "        'מדעים מדויקים': 'טכנולוגיה והנדסה',\n",
        "        'מקצועות המחשב': 'טכנולוגיה והנדסה',\n",
        "        'חקלאות ובעלי חיים': 'מדעי החיים'\n",
        "    }\n",
        "    df['subject'] = df['subject'].replace(mapping)\n",
        "\n",
        "    # 3. Exclusions\n",
        "    # Note: 'חקלאות ובעלי חיים' was mapped to 'מדעי החיים' above.\n",
        "    # Since 'מדעי החיים' is in the exclusion list, those rows will be removed.\n",
        "    exclusions = [\"בלטי מקצועי\", \"בלתי מקצועי\", \"לא עבד\", \"לא צויין\", \"עצמאי\", \"מדעי החיים\"]\n",
        "    df_clean = df[~df['subject'].isin(exclusions)]\n",
        "\n",
        "    # 4. Grouping (The Aggregation Step)\n",
        "    # We count how many people moved from specific Country -> Subject\n",
        "    df_aggregated = df_clean.groupby(['erez_moza', 'subject']).size().reset_index(name='count')\n",
        "\n",
        "    # 5. Save\n",
        "    print(f\"Original rows: {len(df)}\")\n",
        "    print(f\"Aggregated rows: {len(df_aggregated)}\")\n",
        "    df_aggregated.to_csv(OUTPUT_PATH, index=False)\n",
        "    print(f\"Saved aggregated data to {OUTPUT_PATH}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    process_and_save_data()"
      ],
      "metadata": {
        "id": "_VAIMOS-mSi-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}